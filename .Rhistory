readRDS("data1201.rds")
data=readRDS("data1201.rds")
View(data)
load("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/Sunyi/data1201.RData")
data[,c(1,44:50)]
data=data[,c(1,44:50)]
View(data)
saveRDS("data1204.rds")
saveRDS(data,"data1204.rds")
require(rjags)
prior_model_Weibull = "model{
for (i in 1:N){
In_Book[i] ~ dweib(alpha,beta)
}
alpha ~ dunif(0,100)
beta ~ dunif(0,100)
theta <- pow(-log(0.5)/beta,1/alpha)
lambda <- exp(-beta*pow(271,alpha))
}"
prior_model.con_W<-textConnection(prior_model_Weibull)
prior_model.con_W
In_Book_15 = as.integer(data$In_Book)
N_15 = length(data$In_Book)
jags15 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_15, 'N' = N_15))
sample15 = coda.samples(jags15, c("alpha","beta","theta", "lambda"), 10000)
#quartz()
plot(sample15, trace = FALSE)
In_Book_15
N_15
View(data)
jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_15, 'N' = N_15))
list('In_Book' = In_Book_15, 'N' = N_15)
prior_model.con_W
prior_model_Weibull = "model{
for (i in 1:N){
In_Book[i] ~ dweib(alpha,beta)
}
alpha ~ dunif(0,100)
beta ~ dunif(0,100)
theta <- pow(-log(0.5)/beta,1/alpha)
lambda <- exp(-beta*pow(150,alpha))
}"
prior_model.con_W<-textConnection(prior_model_Weibull)
In_Book_14 = as.integer(data2$In_Book)
N_14 = length(data2$In_Book)
jags14 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_14, 'N' = N_14))
data$In_Book_15 = as.integer(data$In_Book)
View(data)
jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_15, 'N' = N_15))
list('In_Book' = In_Book_15, 'N' = N_15)
load("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/Sunyi/data1201.RData")
View(data)
setwd("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/")
data=readRDS("updated_data.rds")
View(data)
data=data[,c(1,44:51)]
View(data)
saveRDS(data,"data1204.rds")
data=readRDS("data1204.rds")
data=readRDS("data1204.rds")
View(data)
data=data[,c(4,5)]
prior_model_Weibull = "model{
for (i in 1:N){
In_Book[i] ~ dweib(alpha,beta)
}
alpha ~ dunif(0,100)
beta ~ dunif(0,100)
theta <- pow(-log(0.5)/beta,1/alpha)
lambda <- exp(-beta*pow(271,alpha))
}"
prior_model.con_W<-textConnection(prior_model_Weibull)
data$In_Book_15 = as.integer(data$In_Book)
N_15 = length(data$In_Book)
jags15 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_15, 'N' = N_15))
data=readRDS("data1204.rds")
View(data)
data=data[,c(4,7,9)]
prior_model_Weibull = "model{
for (i in 1:N){
In_Book[i] ~ dweib(alpha,beta)
}
alpha ~ dunif(0,100)
beta ~ dunif(0,100)
theta <- pow(-log(0.5)/beta,1/alpha)
lambda <- exp(-beta*pow(271,alpha))
}"
prior_model.con_W<-textConnection(prior_model_Weibull)
data$In_Book_15 = as.integer(data$In_Book)
N_15 = length(data$In_Book)
jags15 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_15, 'N' = N_15))
View(data)
data=data[-negative,]
negative<-c(which(data$In_Book < 0),which(is.na(data$In_Book)))
data=data[-negative,]
prior_model_Weibull = "model{
for (i in 1:N){
In_Book[i] ~ dweib(alpha,beta)
}
alpha ~ dunif(0,100)
beta ~ dunif(0,100)
theta <- pow(-log(0.5)/beta,1/alpha)
lambda <- exp(-beta*pow(271,alpha))
}"
prior_model.con_W<-textConnection(prior_model_Weibull)
data$In_Book_15 = as.integer(data$In_Book)
N_15 = length(data$In_Book)
jags15 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_15, 'N' = N_15))
data$In_Book[312]
setwd("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/")
data=readRDS("data1204.rds")
require(randomForest)
require(ggplot2)
require(gridExtra)
require(caret)
library(MASS)
d=data
library(survival)
survreg(Surv(data$In_Book[-negative])~1)
ll.weibull<-function(dat,par){
a=exp(par[1])
b=exp(par[2])
ll=-sum(dweibull(dat,a,scale=b,log=T))
}
weibull.optim <- optim(par=c(log(1/2.2),12),fn=ll.weibull,dat=data$In_Book[-negative])
negative<-c(which(data$In_Book < 0),which(is.na(data$In_Book)))
data=data[-negative,]
prior_model_Weibull = "model{
for (i in 1:N){
In_Book[i] ~ dweib(alpha,beta)
}
alpha ~ dunif(0,100)
beta ~ dunif(0,100)
theta <- pow(-log(0.5)/beta,1/alpha)
lambda <- exp(-beta*pow(271,alpha))
}"
prior_model.con_W<-textConnection(prior_model_Weibull)
data$In_Book_15 = as.integer(data$In_Book)
N_15 = length(data$In_Book)
jags15 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = In_Book_15, 'N' = N_15))
data=readRDS("data1204.rds")
View(data)
data$In_Book_15 = as.integer(data$In_Book)
N_15 = length(data$In_Book)
jags15 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = data$In_Book_15, 'N' = N_15))
setwd("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/")
data=readRDS("data1204.rds")
require(rjags)
negative<-c(which(data$In_Book < 0),which(is.na(data$In_Book)))
data=data[-negative,]
prior_model_Weibull = "model{
for (i in 1:N){
In_Book[i] ~ dweib(alpha,beta)
}
alpha ~ dunif(0,100)
beta ~ dunif(0,100)
theta <- pow(-log(0.5)/beta,1/alpha)
lambda <- exp(-beta*pow(271,alpha))
}"
prior_model.con_W<-textConnection(prior_model_Weibull)
data$In_Book_15 = as.integer(data$In_Book)
N_15 = length(data$In_Book)
jags15 = jags.model(prior_model.con_W, n.chains = 3, n.adapt = 200, data = list('In_Book' = data$In_Book_15, 'N' = N_15))
sample15 = coda.samples(jags15, c("alpha","beta","theta", "lambda"), 10000)
#quartz()
plot(sample15, trace = FALSE)
summary(sample15)
gelman.diag(sample15) # Gelman and Rubin Convergence Diagnostic
sample15
summary(sample15)
jags15
setwd("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/")
data=readRDS("data1204.rds")
#d <- read.csv("character-deaths.csv")
require(randomForest)
require(ggplot2)
require(gridExtra)
require(caret)
library(MASS)
g = function(x){
return(dweibull(x,shape = 1.8,scale = 1.5))
}
ggplot(data.frame(x = data$In_Book),aes(x=x)) +
geom_histogram(aes(y=..density..),binwidth=1) +
stat_function(fun = g, colour = "red",n=10000)+
ggtitle("Density of Weibull with shape 1.75 and scale 1.5")+xlim(c(0,10))
p1=ggplot(data.frame(x = data$In_Book),aes(x=x)) +
geom_histogram(aes(y=..density..),binwidth=4) +
ggtitle("Density of surviving chapters")
p1
library(survival)
set.seed(456)
x <- matrix(sample(0:1, size = 20000, replace = TRUE), ncol = 2)
dCox <- dataCox(10^4, lambda = 3, rho = 2, x,
beta = c(2,2), cens.rate = 5)
head(dCox)
dataCox <- function(n, lambda, rho, x, beta, cens.rate){
# real Weibull times
u <- runif(n)
Treal <- (- log(u) / (lambda * exp(x %*% beta)))^(1 / rho)
# censoring times
Censoring <- rexp(n, cens.rate)
# follow-up times and event indicators
time <- pmin(Treal, Censoring)
status <- as.numeric(Treal <= Censoring)
# data set
data.frame(id = 1:n, time = time, status = status, x = x)
}
coxphSGDprepare <- function(formula, data) {
# Parameter identification as in  `survival::coxph()`.
Call <- match.call()
indx <- match(c("formula", "data"), names(Call), nomatch = 0)
if (indx[1] == 0)
stop("A formula argument is required")
temp <- Call[c(1, indx)]
temp[[1]] <- as.name("model.frame")
mf <- eval(temp, parent.frame())
Y <- model.extract(mf, "response")
if (!inherits(Y, "Surv"))
stop("Response must be a survival object")
type <- attr(Y, "type")
if (type != "right" && type != "counting")
stop(paste("Cox model doesn't support \"", type, "\" survival data",
sep = ""))
# collect times, status, variables and reorder samples
# to make the algorithm more clear to read and track
cbind(event = unclass(Y)[,2], # 1 indicates event, 0 indicates cens   //cbind=>c++: https://stackoverflow.com/questions/31913437/r-fast-cbind-matrix-using-rcpp
times = unclass(Y)[,1],
mf[, -1]) -> data2return
data2return[order(data2return$times), ] # dplyr::arrange(times)   //order=>sort algorithm
}
coxphSGDbatch <- function(formula, data, learning.rate, beta){
# collect times, status, variables and reorder samples
# to make the algorithm more clear to read and track
batchData <- coxphSGDprepare(formula = formula, data = data) # sorts times lol
# calculate the log-likelihood for this batch sample
batchData <- batchData[order(-batchData$times), ] # dplyr::arrange(-times) / sorts time again but with different order
# scores occure in nominator and denominator
scores <- apply(batchData[, -c(1, 2)], 1,
function(element) exp(element %*% beta))
nominator <- apply(batchData[, -c(1, 2)], 2,
function(element) cumsum(scores*element) )   #apply=>c++ matrix calculation
denominator <- cumsum(scores)
# sum over non-censored observations
partial_sum <- (batchData[, -c(1, 2)] - nominator/denominator)*batchData[, "event"]  #c++ matrix
# each column indicates one explanatory variable
U_batch <- colSums(partial_sum)
return(beta + learning.rate * U_batch)
}
coxphSGDcheck <- function(formula, data, learn.rates,
beta.zero, epsilon) {
stopifnot(is.list(data) & length(data) > 0)
stopifnot(length(unique(unlist(lapply(data, ncol)))) == 1)
# + check names and types for every variables
stopifnot(is.function(learn.rates))
stopifnot(is.numeric(epsilon))
stopifnot(is.numeric(beta.zero))
# check length of the start parameter
if (length(beta.zero) == 1) {
beta.zero <-
rep(beta.zero,
length(
unlist(
strsplit(
as.character(
formula
)[3],
split = "\\+")
)
)
)
}
return(beta.zero)
}
coxphSGD <- function(formula, data, learn.rates = function(x){1/x},  #learn.rates function 使learning rate随interation递减，除了1/x还有其他形式
beta.zero = 0, epsilon = 1e-5, max.iter = 500,
verbose = FALSE) {
# check arguments
beta_start <-
coxphSGDcheck(
formula,
data,
learn.rates,
beta.zero,
epsilon
)
n <- length(data)
diff <- epsilon + 1
i <- 1
beta_new <- list()     # steps are saved in a list so that they can
beta_old <- beta_start # be traced in the future
# estimate
while(i <= max.iter & diff > epsilon) {
beta_new[[i]] <-
unlist(
coxphSGDbatch(
formula = formula,
beta = beta_old,
learning.rate = learn.rates(i),
data = data[[ifelse(i%%n==0, n, i%%n)]]   #
)
)
diff <- sqrt(sum((beta_new[[i]] - beta_old)^2))
beta_old <- beta_new[[i]]
i <- i + 1
if (verbose) {
cat("\r iteration: ", i, "\r")
}
}
# return results
list(
Call = match.call(),
epsilon = epsilon,
learn.rates = learn.rates,
steps = i,
coefficients = c(list(beta_start), beta_new)
)
}
dCox <- dataCox(10^4, lambda = 3, rho = 2, x,
beta = c(2,2), cens.rate = 5)
head(dCox)
batch_id <- sample(1:90, size = 10^4, replace = TRUE)
dCox_split <- split(dCox, batch_id)
coxphSGDprepare
dCox_split
a=dCox_split
a[1]
b=a[1]
b=a[[1]]
View(b)
coxphSGDprepare(formula     = Surv(time, status) ~ x.1+x.2,
data        = b)
c=coxphSGDprepare(formula     = Surv(time, status) ~ x.1+x.2,
data        = b)
View(c)
View(b)
View(c)
load("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/Sunyi/data1201.RData")
View(data)
data=readRDS("data1204.rds")
View(data)
data=readRDS("reshapedata.rds")
data=readRDS("reshaped.rds")
setwd("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/")
data=readRDS("reshaped.rds")
data=readRDS("reshaped.rds")
View(data)
load("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/Sunyi/data1201.RData")
View(data)
data=readRDS("data1201.rds")
View(data)
load("/Users/sunyichi/Documents/GitHub/GoT-Bayesian/Sunyi/data1201.RData")
View(data)
data$house=="Night's Watch"
data[data$house=="Night's Watch",47]
sum(data$house=="Night's Watch",na.rm = TRUE)
A=data$house=="Night's Watch"
A==TRUE
A==1
data[c(6,11,42,56,61,63,65,83,84,88,92,97,107,113,119,126,127,133,136,142,149,157,172,174,188,197,198,199,219,236,243,248,249,250,267,280,286,288,299,301,308,312,327,333,359,362,368,400,404,421,424,425,448,464,484,488,490,531,536,559,561,565,575,607,610,631,676,678,685,690,692,707,708,710,712,719,721,744,747,753,766,803,807,819,846,847),47]
data[c(6,11,42,56,61,63,65,83,84,88,92,97,107,113,119,126,127,133,136,142,149,157,172,174,188,197,198,199,219,236,243,248,249,250,267,280,286,288,299,301,308,312,327,333,359,362,368,400,404,421,424,425,448,464,484,488,490,531,536,559,561,565,575,607,610,631,676,678,685,690,692,707,708,710,712,719,721,744,747,753,766,803,807,819,846,847),47]="North"
View(data)
sum(is.na(data[,47]))
saveRDS(data,"fulldata1204.rds")
data=data[,c(1,44:50)]
saveRDS(data,"reduceddata1204.rds")
trainCox<- archivist::aread('MarcinKosinski/MasterThesis/1a06bef4a60a237bb65ca3e2f3f23515')
a=trainCox[[1]]
a=trainCox[[2]]
a=trainCox[[3]]
a=trainCox[[30]]
a=trainCox[[56]]
nrows(trainCox[[56]])
nrow(trainCox[[56]])
sum=0
for(i in 1:98){sum=sum+nrow(trainCox[[i]])}
sum
testCox <- archivist::aread('MarcinKosinski/MasterThesis/3eebc99bd231b16a3ea4dbeec9ab5edb')
length(testCox)
nrow(trainCox[[1]])+nrow(trainCox[[2]])
5973+121
head(
trainCox[[1]][ # train data is a list of data frames that corresponds to batches
c(1:10),    # pick few rows
c(1085:1093) # pick few columns, and survival outcomes
]
)
